{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "##Data Augmentation##\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.5),\n",
    "    transforms.RandomAffine(degrees=40),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "test_set=torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = 4, shuffle=False, num_workers = 2)\n",
    "\n",
    "train_set=torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 4, shuffle=False, num_workers = 2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "class Convolutial_Net(nn.Module):\n",
    "  def __init__(model):\n",
    "    super(Convolutial_Net, model).__init__()\n",
    "    \n",
    "    model.conv1 = nn.Conv2d(3,84,kernel_size=7,stride=1,padding=2)\n",
    "    model.conv1_BN = nn.BatchNorm2d(84)\n",
    "    model.pooling1 = nn.MaxPool2d(3,1)\n",
    "    \n",
    "    model.conv2 = nn.Conv2d(84,250,kernel_size=7,stride=1,padding=2)\n",
    "    model.conv2_BN = nn.BatchNorm2d(250)\n",
    "    model.pooling2 = nn.MaxPool2d(3,2)\n",
    "    \n",
    "    model.conv3 = nn.Conv2d(250,375,kernel_size=5,stride=1,padding=2)\n",
    "    model.conv3_BN = nn.BatchNorm2d(375)\n",
    "    \n",
    "    model.conv4 = nn.Conv2d(375,375,kernel_size=3,stride=1,padding=2)\n",
    "    model.conv4_BN = nn.BatchNorm2d(375)\n",
    "    \n",
    "    model.conv5 = nn.Conv2d(375,250,kernel_size=3,stride=1,padding=2)\n",
    "    model.conv5_BN = nn.BatchNorm2d(250)\n",
    "    model.pooling3 = nn.MaxPool2d(3,2)\n",
    "    \n",
    "    model.fc1 = nn.Linear(7*7*250,4096)            ##여기 64000 값을 좀 줄일 수 있도록 Model 구성해야함\n",
    "    model.fc1_BN = nn.BatchNorm1d(4096)\n",
    "    \n",
    "    model.fc2 = nn.Linear(4096,1024)\n",
    "    model.fc2_BN = nn.BatchNorm1d(1024)\n",
    "    \n",
    "    model.fc3 = nn.Linear(1024,10)  #classification 10 kinds of image\n",
    "    \n",
    "    model.dp = nn.Dropout(p=0.5)\n",
    "    \n",
    "  def forward(model,x):\n",
    "      x = model.pooling1(f.relu(model.conv1_BN(model.conv1(x))))\n",
    "      x = model.pooling2(f.relu(model.conv2_BN(model.conv2(x))))\n",
    "      \n",
    "      x = f.relu(model.conv3_BN(model.conv3(x)))\n",
    "      x = f.relu(model.conv4_BN(model.conv4(x)))\n",
    "      x= model.pooling3(f.relu(model.conv5_BN(model.conv5(x))))\n",
    "      \n",
    "      \n",
    "      ##Making Fully Connection and dropout\n",
    "      x = x.view(x.size(0),-1)  \n",
    "      x = model.dp(x)\n",
    "      \n",
    "      x = f.relu(model.fc1_BN(model.fc1(x)))\n",
    "      x = model.dp(x)\n",
    "      \n",
    "      x = f.relu(model.fc2_BN(model.fc2(x)))\n",
    "      x = model.dp(x)\n",
    "      \n",
    "      x = f.log_softmax(model.fc3(x), dim = 0)\n",
    "      \n",
    "      return x  \n",
    "      \n",
    "net =  Convolutial_Net().to(device)       \n",
    "##initializing weights with HeNormal##      \n",
    "def init_weights(module) : \n",
    "    \n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        torch.nn.init.kaiming_uniform_(module.weight)\n",
    "        torch.nn.init.kaiming_uniform_(module.bias)\n",
    "        \n",
    "    elif isinstance(module, nn.BatchNorm1d):\n",
    "        torch.nn.init.kaiming_uniform_(module.weight)\n",
    "        torch.nn.init.kaiming_uniform_(module.bias)\n",
    "             \n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        torch.nn.init.kaiming_uniform_(module.weight)\n",
    "        torch.nn.init.kaiming_uniform_(module.bias)\n",
    "    \n",
    "    elif isinstance(module, nn.Linear):\n",
    "        torch.nn.init.kaiming_uniform_(module.weight)    \n",
    "        torch.nn.init.kaiming_uniform_(module.bias)         \n",
    "        \n",
    "  \n",
    "init_weights(net)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(net.parameters(),lr=0.1)\n",
    "\n",
    "epoch_loss_list=[]\n",
    "epoch_acc_list=[]\n",
    "valid_loss_list=[]\n",
    "valid_acc_list=[]\n",
    "\n",
    "\n",
    "##Training our net##\n",
    "for epoch in range(100):   # 데이터셋을 수차례 반복합니다.\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc = 0.0\n",
    "    epoch_loss=0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "\n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # [inputs, labels]의 목록인 data로부터 입력을 받은 후;\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # 변화도(Gradient) 매개변수를 0으로 만들고\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 순전파 + 역전파 + 최적화를 한 후\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(axis=1)==labels).float().sum()/10\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 통계를 출력합니다.\n",
    "        running_loss += loss.item()\n",
    "        running_acc += acc.item()\n",
    "        if i % 10 == 9:    # print every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f acc: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10, running_acc/10))\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            \n",
    "    print('[%d, %5d] Finish loss : %.3f acc: %.3f' %(\n",
    "        epoch+1, i+1, epoch_loss / (i+1), epoch_acc / (i+1)\n",
    "    ))\n",
    "    \n",
    "    epoch_loss_list.append(epoch_loss/(i+1))\n",
    "    epoch_acc_list.append(epoch_acc/(i+1))\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc =0.0\n",
    "    net.eval()\n",
    "    \n",
    "    for i, data in enumerate(test_loader,0):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = (outputs.argmax(axis=1)==labels).float().sum()/10\n",
    "        valid_loss+=loss.item()\n",
    "        valid_acc +=acc.item()\n",
    "        \n",
    "    print('[%d, %5d] Valid loss : %.3f acc: %.3f \\n' %(\n",
    "        epoch+1, i+1, valid_loss / (i+1), valid_acc / (i+1)\n",
    "    ))    \n",
    "    \n",
    "    valid_loss_list.append(valid_loss/(i+1))\n",
    "    valid_acc_list.append(valid_acc/(i+1))\n",
    "    \n",
    "print('Finished Training')    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
